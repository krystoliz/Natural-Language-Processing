{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK library - natural language toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization with NLTK\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jhon samuel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jhon samuel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jhon samuel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jhon samuel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jhon samuel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jhon samuel\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jhon\n",
      "[nltk_data]     Samuel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jhon Samuel/nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m scene_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCENE 1: [wind] [clop clop clop] \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKING ARTHUR: Whoa there!  [clop clop clop] \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Halt!  Who goes there?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Pull the other one!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: What?  Ridden on a horse?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: Yes!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using coconuts!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: What?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve got two empty halves of coconut and you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre bangin\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mem together.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Where\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md you get the coconuts?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: We found them.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Found them?  In Mercea?  The coconut\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms tropical!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: What do you mean?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Well, this is a temperate zone.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Are you suggesting coconuts migrate?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: Not at all.  They could be carried.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: What?  A swallow carrying a coconut?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: It could grip it by the husk!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not a question of where he grips it!  It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: Well, it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: Please!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Am I right?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm not interested!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: It could be carried by an African swallow!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms my point.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: Oh, yeah, I agree with that.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mARTHUR: Will you ask your master if he wants to join my court at Camelot?!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: But then of course a-- African swallows are non-migratory.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: Oh, yeah...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: So they couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt bring a coconut back anyway...  [clop clop clop] \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: No, they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md have to have it on a line.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: Well, simple!  They\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md just use a strand of creeper!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #1: What, held under the dorsal guiding feathers?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLDIER #2: Well, why not?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Split scene_one into sentences: sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenized_sent \u001b[38;5;241m=\u001b[39m word_tokenize(sentences[\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Jhon Samuel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Jhon Samuel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhon Samuel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jhon Samuel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Jhon Samuel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jhon Samuel/nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jhon Samuel\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# scene_one is a string that contains the first scene of \"Monty Python and the Holy Grail\"\n",
    "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More regex with re.search()\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\w => words\n",
    "\\s => whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a tokenizer\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "\n",
    "```\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "```\n",
    "\n",
    "The string is available in your workspace as ```my_string```, and the patterns have been pre-loaded as ```pattern1```, ```pattern2 ```, ```pattern3```, and     ```pattern4```, respectively.\n",
    "\n",
    "Additionally, ```regexp_tokenize``` has been imported from ```nltk.tokenize```. You can use ```regexp_tokenize(string, pattern)``` with ```my_string``` and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| => or\n",
    "\\\\ => escape special characters in regex\n",
    "\\\\w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\w+ => greedy quantifier but allow backtracking so it doesnt entirely fail\n",
    "\\\\w++ => possesive quantifier that doesnt allow backtracking so it entirely fail\n",
    "\n",
    "see pattern 5 and t6 below for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mynameis123456']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "string2 = \"mynameis123456\"\n",
    "pattern1 = '(\\\\w+|\\\\?|!)'\n",
    "pattern2= '(\\\\w+|#\\\\d|\\\\?|!)'\n",
    "pattern3 = '(#\\\\d\\\\w+\\\\?!)'\n",
    "pattern4 = '\\\\s+'\n",
    "pattern5 = '(\\\\w+\\\\d)'\n",
    "pattern6 = '(\\\\w++\\\\d)'\n",
    "\n",
    "print(regexp_tokenize(string2, pattern5))\n",
    "print(regexp_tokenize(string2, pattern6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(my_string, pattern3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
    "\n",
    "Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[2], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "german_text ='Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
